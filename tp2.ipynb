{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implemente un perceptrón simple que aprenda la función lógica $AND$ y la función lógica $OR$, de $2$ y de $4$ entradas. Muestre la evolución del error durante el entrenamiento. Para el caso de $2$ dimensiones, grafique la recta discriminadora y todos los vectores de entrada de la red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/perceptrón-simple1.png)\n",
    "\n",
    "$AND$ de $2$ entradas:\n",
    "| $x_1$ | $0$ | $0$ | $1$ | $1$ |\n",
    "|-------|-----|-----|-----|-----|\n",
    "| $x_2$ | $0$ | $1$ | $0$ | $1$ |\n",
    "| $y$   | $0$ | $0$ | $0$ | $1$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "np.random.seed(2002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AND(X):\n",
    "    return all(X)\n",
    "\n",
    "def OR(X):\n",
    "    return any(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronSimple:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(3)\n",
    "    def train(self, X, Y, alpha, iter_):\n",
    "        for _ in range(iter_):\n",
    "            for n in range(len(X)):\n",
    "                a = self.predict(X[n])\n",
    "                if a != Y[n]:\n",
    "                    self.W[0] += alpha * (Y[n] - a) * X[n][0]\n",
    "                    self.W[1] += alpha * (Y[n] - a) * X[n][1]\n",
    "                    self.W[2] += alpha * (Y[n] - a) * (-1)\n",
    "    def predict(self, x):\n",
    "        h = np.dot(np.append(x, -1), self.W)\n",
    "        return 0 if h < 0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0] 0\n",
      "[0, 1] 0\n",
      "[1, 0] 0\n",
      "[1, 1] 1\n"
     ]
    }
   ],
   "source": [
    "X_train = [[x1,x2] for x1 in [0,1] for x2 in[0,1]]\n",
    "Y_train = [AND(x) for x in X_train]\n",
    "\n",
    "perceptron = PerceptronSimple()\n",
    "perceptron.train(X_train[1:] + [[1,1]], Y_train[1:] + [True], 0.01, 10000)\n",
    "\n",
    "for x in X_train:\n",
    "    print(x, perceptron.predict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$AND$ de $4$ entradas:\n",
    "\n",
    "| $x_1$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $1$ | $1$ | $1$ | $1$ | $1$ | $1$ | $1$ | $1$ |\n",
    "|-------|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|\n",
    "| $x_2$ | $0$ | $0$ | $0$ | $0$ | $1$ | $1$ | $1$ | $1$ | $0$ | $0$ | $0$ | $0$ | $1$ | $1$ | $1$ | $1$ |\n",
    "| $x_3$ | $0$ | $0$ | $1$ | $1$ | $0$ | $0$ | $0$ | $1$ | $0$ | $0$ | $1$ | $1$ | $0$ | $0$ | $1$ | $1$ |\n",
    "| $x_4$ | $0$ | $1$ | $0$ | $1$ | $0$ | $1$ | $0$ | $1$ | $0$ | $1$ | $0$ | $1$ | $0$ | $1$ | $0$ | $1$ |\n",
    "| $y$   | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $1$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronSimple:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(5)\n",
    "    def train(self, X, Y, alpha, iter_):\n",
    "        for _ in range(iter_):\n",
    "            for n in range(len(X)):\n",
    "                a = self.predict(X[n])\n",
    "                if a != Y[n]:\n",
    "                    self.W[0] += alpha * (Y[n] - a) * X[n][0]\n",
    "                    self.W[1] += alpha * (Y[n] - a) * X[n][1]\n",
    "                    self.W[2] += alpha * (Y[n] - a) * X[n][2]\n",
    "                    self.W[3] += alpha * (Y[n] - a) * X[n][3]\n",
    "                    self.W[4] += alpha * (Y[n] - a) * (-1)\n",
    "    def predict(self, x):\n",
    "        h = np.dot(np.append(x, -1), self.W)\n",
    "        return 0 if h < 0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0] 0\n",
      "[0, 0, 0, 1] 0\n",
      "[0, 0, 1, 0] 0\n",
      "[0, 0, 1, 1] 0\n",
      "[0, 1, 0, 0] 0\n",
      "[0, 1, 0, 1] 0\n",
      "[0, 1, 1, 0] 0\n",
      "[0, 1, 1, 1] 0\n",
      "[1, 0, 0, 0] 0\n",
      "[1, 0, 0, 1] 0\n",
      "[1, 0, 1, 0] 0\n",
      "[1, 0, 1, 1] 0\n",
      "[1, 1, 0, 0] 0\n",
      "[1, 1, 0, 1] 0\n",
      "[1, 1, 1, 0] 0\n",
      "[1, 1, 1, 1] 1\n"
     ]
    }
   ],
   "source": [
    "X_train = [[x1,x2,x3,x4] for x1 in [0,1] for x2 in[0,1] for x3 in [0,1] for x4 in[0,1]]\n",
    "Y_train = [AND(x) for x in X_train]\n",
    "\n",
    "perceptron = PerceptronSimple()\n",
    "perceptron.train(X_train[5:] + [[1,1,1,1] for _ in range(5)], Y_train[5:] + [True for _ in range(5)], 0.001, 10000)\n",
    "\n",
    "for x in X_train:\n",
    "    print(x, perceptron.predict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Implemente un perceptrón multicapa que aprenda la función lógica $XOR$ de $2$ y de $4$ entradas (utilizando el algoritmo Backpropagation y actualizando en batch). Muestre cómo evoluciona el error durante el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/perceptrón-multicapa1.png)\n",
    "\n",
    "$XOR$ de $2$ entradas:\n",
    "| $x_1$ | $0$ | $0$ | $1$ | $1$ |\n",
    "|-------|-----|-----|-----|-----|\n",
    "| $x_2$ | $0$ | $1$ | $0$ | $1$ |\n",
    "| $y$   | $0$ | $1$ | $1$ | $0$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def d_sigmoid(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def d_tanh(z):\n",
    "    return 1 - np.tanh(z)**2\n",
    "\n",
    "class PerceptronMulticapa:\n",
    "    def __init__(self, sizes, g=sigmoid, d_g=d_sigmoid):\n",
    "        self.L = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.a = [[0 for _ in range(s)] for s in sizes]\n",
    "        self.z = [[0 for _ in range(s)] for s in sizes[1:]]\n",
    "        self.w = [np.random.randn(n,m) for n, m in zip(sizes[1:], sizes[:-1])]\n",
    "        self.b = [np.random.randn(s) for s in sizes[1:]]\n",
    "        self.g = g\n",
    "        self.d_g = d_g\n",
    "    def predict(self, x):\n",
    "        self.a[0] = x\n",
    "        for l in range(1, self.L):\n",
    "            self.z[l-1] = self.w[l-1] @ self.a[l-1] + self.b[l-1]\n",
    "            self.a[l] = self.g(self.z[l-1])\n",
    "        return self.a[-1]\n",
    "\n",
    "    def train(self, X, Y, lr=0.01, iters=1000):\n",
    "        for _ in range(iters):  # Iterar varias veces\n",
    "            for x, y in zip(X, Y):\n",
    "                # predigo x para actualizar la matriz de activaciones\n",
    "                self.predict(x)\n",
    "                \n",
    "                # calculo el error\n",
    "                grad_C_a = self.a[-1] - y\n",
    "                delta_l = grad_C_a * self.d_g(self.z[-1])\n",
    "                \n",
    "                # inicializo los gradientes\n",
    "                grad_C_w = [np.zeros_like(w) for w in self.w]\n",
    "                grad_C_b = [np.zeros_like(b) for b in self.b]\n",
    "                \n",
    "                # backprop\n",
    "                for l in range(self.L-2, -1, -1):\n",
    "                    grad_C_w[l] = np.outer(delta_l, self.a[l])\n",
    "                    grad_C_b[l] = delta_l\n",
    "                    if l > 0:\n",
    "                        delta_l = (self.w[l].T @ delta_l) * self.d_g(self.z[l-1])\n",
    "                \n",
    "                # muevo los parámetros en la dirección contraria al gradiente en módulo learning rate\n",
    "                self.w = [w - lr * grad_w for w, grad_w in zip(self.w, grad_C_w)]\n",
    "                self.b = [b - lr * grad_b for b, grad_b in zip(self.b, grad_C_b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0] 0\n",
      "[0, 1] 1\n",
      "[1, 0] 1\n",
      "[1, 1] 0\n"
     ]
    }
   ],
   "source": [
    "X_train = [[x1, x2] for x1 in [0, 1] for x2 in [0, 1]]\n",
    "Y_train = [x1 ^ x2 for x1, x2 in X_train]\n",
    "\n",
    "perceptron = PerceptronMulticapa([2,3,1])\n",
    "perceptron.train(X_train, Y_train, lr=0.1, iters=10000)\n",
    "for x in X_train:\n",
    "    print(x, 1 if perceptron.predict(x) > .5 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$XOR$ de $4$ entradas:\n",
    "| $x_1$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $1$ | $1$ | $1$ | $1$ | $1$ | $1$ | $1$ | $1$ |\n",
    "|-------|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|\n",
    "| $x_2$ | $0$ | $0$ | $0$ | $0$ | $1$ | $1$ | $1$ | $1$ | $0$ | $0$ | $0$ | $0$ | $1$ | $1$ | $1$ | $1$ |\n",
    "| $x_3$ | $0$ | $0$ | $1$ | $1$ | $0$ | $0$ | $1$ | $1$ | $0$ | $0$ | $1$ | $1$ | $0$ | $0$ | $1$ | $1$ |\n",
    "| $x_4$ | $0$ | $1$ | $0$ | $1$ | $0$ | $1$ | $0$ | $1$ | $0$ | $1$ | $0$ | $1$ | $0$ | $1$ | $0$ | $1$ |\n",
    "| $y$   | $0$ | $1$ | $1$ | $0$ | $1$ | $0$ | $0$ | $1$ | $1$ | $0$ | $0$ | $1$ | $0$ | $1$ | $1$ | $0$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0] 0 0\n",
      "[0, 0, 0, 1] 1 1\n",
      "[0, 0, 1, 0] 1 1\n",
      "[0, 0, 1, 1] 0 0\n",
      "[0, 1, 0, 0] 1 1\n",
      "[0, 1, 0, 1] 0 0\n",
      "[0, 1, 1, 0] 0 0\n",
      "[0, 1, 1, 1] 1 1\n",
      "[1, 0, 0, 0] 1 1\n",
      "[1, 0, 0, 1] 0 0\n",
      "[1, 0, 1, 0] 0 0\n",
      "[1, 0, 1, 1] 1 1\n",
      "[1, 1, 0, 0] 0 0\n",
      "[1, 1, 0, 1] 1 1\n",
      "[1, 1, 1, 0] 1 1\n",
      "[1, 1, 1, 1] 0 0\n"
     ]
    }
   ],
   "source": [
    "def xor(x):\n",
    "    return x[0] ^ x[1] ^ x[2] ^ x[3]\n",
    "\n",
    "X_train = [[x1, x2, x3, x4] for x1 in [0, 1] for x2 in [0, 1] for x3 in [0, 1] for x4 in [0, 1]]\n",
    "Y_train = [xor(x) for x in X_train]\n",
    "\n",
    "perceptron = PerceptronMulticapa([4,8,1])\n",
    "perceptron.train(X_train,Y_train,lr=0.01,iters=100000)\n",
    "for x in X_train:\n",
    "    print(x, 1 if perceptron.predict(x) > .5 else 0, xor(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\n",
    "    a) Implemente una red con aprendizaje Backpropagation que aprenda la siguiente función:\n",
    "    $$\n",
    "    f(x, y, z) = \\sin(x) + \\cos(y) + z\n",
    "    $$\n",
    "    donde $x, y \\in [0, 2\\pi]$ y $z \\in [-1, 1]$.  \n",
    "    Para ello construya un conjunto de datos de entrenamiento y un conjunto de evaluación. Muestre la evolución del error de entrenamiento y de evaluación en función de las épocas de entrenamiento.\n",
    "\n",
    "    b) Estudie la evolución de los errores durante el entrenamiento de una red con una capa oculta de $30$ neuronas cuando el conjunto de entrenamiento contiene $40$ muestras.  \n",
    "    ¿Qué ocurre si el minibatch tiene tamaño 40? ¿Y si tiene tamaño 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, y, z):\n",
    "    return np.sin(x) + np.cos(y) + z\n",
    "\n",
    "X_train = [[x, y, z] for x, y, z in zip(np.linspace(0, 2*np.pi, 100), np.linspace(0, 2*np.pi, 100), np.linspace(-1, 1, 100))]\n",
    "Y_train = np.array([f(x, y, z) for x, y, z in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voy a tener que cambiar la función costo usando el mse\n",
    "# la arquitectura no debe estar tan mal\n",
    "# me está errando los negativos porque la sigmoide sólo me da > 0\n",
    "# voy a usar tangente hiperbólica\n",
    "# la tanh por lo menos no es > 0 pero está acotada < 1 :(\n",
    "\n",
    "perceptron = PerceptronMulticapa([3,15,15,15,1], g=tanh, d_g=d_tanh)\n",
    "perceptron.train(X_train,Y_train,lr=0.01,iters=10000)\n",
    "\n",
    "# for x, y, z in X_train:\n",
    "#     print(str(round(x, 2))+\"\\t\"+\n",
    "#           str(round(y, 2))+\"\\t\"+\n",
    "#           str(round(z, 2))+\"\\t\"+\n",
    "#           str(round(perceptron.predict([x, y, z])[0],2))+\"\\t\"+\n",
    "#           str(round(f(x, y, z),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04492634565350915"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mse(Y_h, Y):\n",
    "    return sum((Y_h - Y)**2)/len(Y)\n",
    "\n",
    "Y = np.array([perceptron.predict([x, y, z])[0] for x, y, z in X_train])\n",
    "mse(Y_train, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0 0.0\n",
      "0.09 0.08\n",
      "0.16 0.16\n",
      "0.23 0.23\n",
      "0.3 0.3\n",
      "0.37 0.36\n",
      "0.42 0.42\n",
      "0.48 0.47\n",
      "0.52 0.52\n",
      "0.56 0.56\n",
      "0.6 0.6\n",
      "0.63 0.63\n",
      "0.66 0.66\n",
      "0.68 0.68\n",
      "0.69 0.69\n",
      "0.7 0.7\n",
      "0.7 0.7\n",
      "0.69 0.7\n",
      "0.68 0.69\n",
      "0.67 0.67\n",
      "0.65 0.66\n",
      "0.63 0.63\n",
      "0.61 0.6\n",
      "0.57 0.57\n",
      "0.54 0.53\n",
      "0.49 0.49\n",
      "0.45 0.44\n",
      "0.39 0.39\n",
      "0.34 0.34\n",
      "0.28 0.28\n",
      "0.21 0.22\n",
      "0.15 0.16\n",
      "0.09 0.1\n",
      "0.03 0.03\n",
      "-0.03 -0.03\n",
      "-0.1 -0.1\n",
      "-0.16 -0.17\n",
      "-0.23 -0.24\n",
      "-0.3 -0.31\n",
      "-0.38 -0.38\n",
      "-0.45 -0.45\n",
      "-0.52 -0.52\n",
      "-0.59 -0.58\n",
      "-0.65 -0.65\n",
      "-0.7 -0.71\n",
      "-0.76 -0.77\n",
      "-0.82 -0.83\n",
      "-0.89 -0.88\n",
      "-0.94 -0.93\n",
      "-0.97 -0.98\n",
      "-0.99 -1.02\n",
      "-1.0 -1.06\n",
      "-1.0 -1.09\n",
      "-1.0 -1.13\n",
      "-1.0 -1.15\n",
      "-1.0 -1.17\n",
      "-1.0 -1.19\n",
      "-1.0 -1.2\n",
      "-1.0 -1.2\n",
      "-1.0 -1.2\n",
      "-1.0 -1.19\n",
      "-1.0 -1.18\n",
      "-1.0 -1.16\n",
      "-1.0 -1.14\n",
      "-1.0 -1.11\n",
      "-1.0 -1.07\n",
      "-0.99 -1.03\n",
      "-0.97 -0.99\n",
      "-0.94 -0.93\n",
      "-0.9 -0.88\n",
      "-0.84 -0.82\n",
      "-0.77 -0.75\n",
      "-0.69 -0.68\n",
      "-0.6 -0.6\n",
      "-0.52 -0.52\n",
      "-0.43 -0.44\n",
      "-0.34 -0.35\n",
      "-0.25 -0.26\n",
      "-0.15 -0.16\n",
      "-0.05 -0.06\n",
      "0.04 0.04\n",
      "0.14 0.14\n",
      "0.23 0.25\n",
      "0.32 0.35\n",
      "0.43 0.46\n",
      "0.56 0.57\n",
      "0.71 0.68\n",
      "0.85 0.79\n",
      "0.95 0.9\n",
      "0.99 1.01\n",
      "1.0 1.12\n",
      "1.0 1.23\n",
      "1.0 1.33\n",
      "1.0 1.44\n",
      "1.0 1.54\n",
      "1.0 1.64\n",
      "1.0 1.73\n",
      "1.0 1.82\n",
      "1.0 1.91\n",
      "1.0 2.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(Y)):\n",
    "    print(round(Y[i],2), round(Y_train[i],2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Siguiendo el trabajo de Hinton y Salakhutdinov (2006), entrene una máquina restringida\n",
    "de Boltzmann con imágenes de la base de datos MNIST. Muestre el error de\n",
    "recontruccion durante el entrenamiento, y ejemplos de cada uno de los dígitos\n",
    "reconstruidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_322644/707977394.py:18: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from scipy.io import loadmat\n",
    "\n",
    "train = loadmat(\"data/rbm/datosTrain.mat\")[\"data\"]\n",
    "test = loadmat(\"data/rbm/datosTest.mat\")[\"data\"]\n",
    "\n",
    "v_size = 784\n",
    "h_size = 100\n",
    "L = 2\n",
    "\n",
    "# inicializo la red\n",
    "v = np.array([0 for _ in range(v_size)])\n",
    "h = np.array([np.random.randn() for _ in range(h_size)])\n",
    "w = np.random.randn(v_size, h_size)\n",
    "bv = np.random.randn(v_size)\n",
    "bh = np.random.randn(h_size)\n",
    "\n",
    "def E(v, h):\n",
    "    return -bv @ v -bh @ h - v @ w @ h\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# aumentando las probabilidades de los datos de entrenamiento\n",
    "f_vh_data = np.zeros((v_size, h_size)) # fracción de veces que v_i y h_j están activados simultáneamente en los datos\n",
    "f_vh_recon = np.zeros((v_size, h_size)) # misma fracción para las confabulaciones\n",
    "for t in train:\n",
    "    v = t\n",
    "    p_h = sigmoid(bh + v @ w)\n",
    "    h = np.array([1 if np.random.rand() < p else 0 for p in p_h])\n",
    "    f_vh_data += np.outer(v, h)\n",
    "    # confabulaciones\n",
    "    p_v = sigmoid(bv + h @ w.T)\n",
    "    v = np.array([1 if np.random.rand() < p else 0 for p in p_v])\n",
    "    p_h = sigmoid(bh + v @ w)\n",
    "    h = np.array([1 if np.random.rand() < p else 0 for p in p_h])\n",
    "    f_vh_recon += np.outer(v, h)\n",
    "\n",
    "f_vh_data /= len(train)\n",
    "f_vh_recon /= len(train)\n",
    "\n",
    "# acualizo los pesos\n",
    "epsilon = 0.01\n",
    "w += epsilon * (f_vh_data - f_vh_recon)\n",
    "\n",
    "# tengo que actualizar los sesgos también\n",
    "\n",
    "# unfolding\n",
    "def mse(h, y):\n",
    "    return sum((h - y)**2) / len(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Entrene una red convolucional para clasificar las imágenes de la base de datos MNIST.  \n",
    "¿Cuál es la red convolucional más pequeña que puede conseguir con una exactitud de al menos 90% en el conjunto de evaluación? ¿Cuál es el perceptrón multicapa más\n",
    "pequeño que puede conseguir con la misma exactitud?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Entrene un autoencoder para obtener una representación de baja dimensionalidad de las\n",
    "imágenes de MNIST. Use dichas representaciones para entrenar un perceptrón\n",
    "multicapa como clasificador. ¿Cuál es el tiempo de entrenamiento y la exactitud del\n",
    "clasificador obtenido cuando parte de la representación del autoencoder, en\n",
    "comparación con lo obtenido usando las imágenes originales?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
