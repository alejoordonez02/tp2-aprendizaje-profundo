{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implemente un perceptrón simple que aprenda la función lógica $AND$ y la función lógica $OR$, de $2$ y de $4$ entradas. Muestre la evolución del error durante el entrenamiento. Para el caso de $2$ dimensiones, grafique la recta discriminadora y todos los vectores de entrada de la red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/perceptrón-simple1.png)\n",
    "\n",
    "$AND$ de $2$ entradas:\n",
    "| $x_1$ | $0$ | $0$ | $1$ | $1$ |\n",
    "|-------|-----|-----|-----|-----|\n",
    "| $x_2$ | $0$ | $1$ | $0$ | $1$ |\n",
    "| $y$   | $0$ | $0$ | $0$ | $1$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "np.random.seed(2002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AND(X):\n",
    "    return all(X)\n",
    "\n",
    "def OR(X):\n",
    "    return any(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronSimple:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(3)\n",
    "    def train(self, X, Y, alpha, iter_):\n",
    "        for _ in range(iter_):\n",
    "            for n in range(len(X)):\n",
    "                a = self.predict(X[n])\n",
    "                if a != Y[n]:\n",
    "                    self.W[0] += alpha * (Y[n] - a) * X[n][0]\n",
    "                    self.W[1] += alpha * (Y[n] - a) * X[n][1]\n",
    "                    self.W[2] += alpha * (Y[n] - a) * (-1)\n",
    "    def predict(self, x):\n",
    "        h = np.dot(np.append(x, -1), self.W)\n",
    "        return 0 if h < 0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [[x1,x2] for x1 in [0,1] for x2 in[0,1]]\n",
    "Y_train = [AND(x) for x in X_train]\n",
    "\n",
    "perceptron = PerceptronSimple()\n",
    "perceptron.train(X_train[1:] + [[1,1]], Y_train[1:] + [True], 0.01, 10000)\n",
    "\n",
    "for x in X_train:\n",
    "    print(x, perceptron.predict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$AND$ de $4$ entradas:\n",
    "\n",
    "| $x_1$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $1$ | $1$ | $1$ | $1$ | $1$ | $1$ | $1$ | $1$ |\n",
    "|-------|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|\n",
    "| $x_2$ | $0$ | $0$ | $0$ | $0$ | $1$ | $1$ | $1$ | $1$ | $0$ | $0$ | $0$ | $0$ | $1$ | $1$ | $1$ | $1$ |\n",
    "| $x_3$ | $0$ | $0$ | $1$ | $1$ | $0$ | $0$ | $0$ | $1$ | $0$ | $0$ | $1$ | $1$ | $0$ | $0$ | $1$ | $1$ |\n",
    "| $x_4$ | $0$ | $1$ | $0$ | $1$ | $0$ | $1$ | $0$ | $1$ | $0$ | $1$ | $0$ | $1$ | $0$ | $1$ | $0$ | $1$ |\n",
    "| $y$   | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $1$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronSimple:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(5)\n",
    "    def train(self, X, Y, alpha, iter_):\n",
    "        for _ in range(iter_):\n",
    "            for n in range(len(X)):\n",
    "                a = self.predict(X[n])\n",
    "                if a != Y[n]:\n",
    "                    self.W[0] += alpha * (Y[n] - a) * X[n][0]\n",
    "                    self.W[1] += alpha * (Y[n] - a) * X[n][1]\n",
    "                    self.W[2] += alpha * (Y[n] - a) * X[n][2]\n",
    "                    self.W[3] += alpha * (Y[n] - a) * X[n][3]\n",
    "                    self.W[4] += alpha * (Y[n] - a) * (-1)\n",
    "    def predict(self, x):\n",
    "        h = np.dot(np.append(x, -1), self.W)\n",
    "        return 0 if h < 0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [[x1,x2,x3,x4] for x1 in [0,1] for x2 in[0,1] for x3 in [0,1] for x4 in[0,1]]\n",
    "Y_train = [AND(x) for x in X_train]\n",
    "\n",
    "perceptron = PerceptronSimple()\n",
    "perceptron.train(X_train[5:] + [[1,1,1,1] for _ in range(5)], Y_train[5:] + [True for _ in range(5)], 0.001, 10000)\n",
    "\n",
    "for x in X_train:\n",
    "    print(x, perceptron.predict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Implemente un perceptrón multicapa que aprenda la función lógica $XOR$ de $2$ y de $4$ entradas (utilizando el algoritmo Backpropagation y actualizando en batch). Muestre cómo evoluciona el error durante el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/perceptrón-multicapa1.png)\n",
    "\n",
    "$XOR$ de $2$ entradas:\n",
    "| $x_1$ | $0$ | $0$ | $1$ | $1$ |\n",
    "|-------|-----|-----|-----|-----|\n",
    "| $x_2$ | $0$ | $1$ | $0$ | $1$ |\n",
    "| $y$   | $0$ | $1$ | $1$ | $0$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def d_sigmoid(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def d_tanh(z):\n",
    "    return 1 - np.tanh(z)**2\n",
    "\n",
    "class PerceptronMulticapa:\n",
    "    def __init__(self, sizes, g=sigmoid, d_g=d_sigmoid):\n",
    "        self.L = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.a = [[0 for _ in range(s)] for s in sizes]\n",
    "        self.z = [[0 for _ in range(s)] for s in sizes[1:]]\n",
    "        self.w = [np.random.randn(n,m) for n, m in zip(sizes[1:], sizes[:-1])]\n",
    "        self.b = [np.random.randn(s) for s in sizes[1:]]\n",
    "        self.g = g\n",
    "        self.d_g = d_g\n",
    "    def predict(self, x):\n",
    "        self.a[0] = x\n",
    "        for l in range(1, self.L):\n",
    "            self.z[l-1] = self.w[l-1] @ self.a[l-1] + self.b[l-1]\n",
    "            self.a[l] = self.g(self.z[l-1])\n",
    "        return self.a[-1]\n",
    "\n",
    "    def train(self, X, Y, lr=0.01, iters=1000):\n",
    "        for _ in range(iters):  # Iterar varias veces\n",
    "            for x, y in zip(X, Y):\n",
    "                # predigo x para actualizar la matriz de activaciones\n",
    "                self.predict(x)\n",
    "                \n",
    "                # calculo el error\n",
    "                grad_C_a = self.a[-1] - y\n",
    "                delta_l = grad_C_a * self.d_g(self.z[-1])\n",
    "                \n",
    "                # inicializo los gradientes\n",
    "                grad_C_w = [np.zeros_like(w) for w in self.w]\n",
    "                grad_C_b = [np.zeros_like(b) for b in self.b]\n",
    "                \n",
    "                # backprop\n",
    "                for l in range(self.L-2, -1, -1):\n",
    "                    grad_C_w[l] = np.outer(delta_l, self.a[l])\n",
    "                    grad_C_b[l] = delta_l\n",
    "                    if l > 0:\n",
    "                        delta_l = (self.w[l].T @ delta_l) * self.d_g(self.z[l-1])\n",
    "                \n",
    "                # muevo los parámetros en la dirección contraria al gradiente en módulo learning rate\n",
    "                self.w = [w - lr * grad_w for w, grad_w in zip(self.w, grad_C_w)]\n",
    "                self.b = [b - lr * grad_b for b, grad_b in zip(self.b, grad_C_b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [[x1, x2] for x1 in [0, 1] for x2 in [0, 1]]\n",
    "Y_train = [x1 ^ x2 for x1, x2 in X_train]\n",
    "\n",
    "perceptron = PerceptronMulticapa([2,3,1])\n",
    "perceptron.train(X_train, Y_train, lr=0.1, iters=10000)\n",
    "for x in X_train:\n",
    "    print(x, 1 if perceptron.predict(x) > .5 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$XOR$ de $4$ entradas:\n",
    "| $x_1$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $1$ | $1$ | $1$ | $1$ | $1$ | $1$ | $1$ | $1$ |\n",
    "|-------|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|\n",
    "| $x_2$ | $0$ | $0$ | $0$ | $0$ | $1$ | $1$ | $1$ | $1$ | $0$ | $0$ | $0$ | $0$ | $1$ | $1$ | $1$ | $1$ |\n",
    "| $x_3$ | $0$ | $0$ | $1$ | $1$ | $0$ | $0$ | $1$ | $1$ | $0$ | $0$ | $1$ | $1$ | $0$ | $0$ | $1$ | $1$ |\n",
    "| $x_4$ | $0$ | $1$ | $0$ | $1$ | $0$ | $1$ | $0$ | $1$ | $0$ | $1$ | $0$ | $1$ | $0$ | $1$ | $0$ | $1$ |\n",
    "| $y$   | $0$ | $1$ | $1$ | $0$ | $1$ | $0$ | $0$ | $1$ | $1$ | $0$ | $0$ | $1$ | $0$ | $1$ | $1$ | $0$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xor(x):\n",
    "    return x[0] ^ x[1] ^ x[2] ^ x[3]\n",
    "\n",
    "X_train = [[x1, x2, x3, x4] for x1 in [0, 1] for x2 in [0, 1] for x3 in [0, 1] for x4 in [0, 1]]\n",
    "Y_train = [xor(x) for x in X_train]\n",
    "\n",
    "perceptron = PerceptronMulticapa([4,8,1])\n",
    "perceptron.train(X_train,Y_train,lr=0.01,iters=100000)\n",
    "for x in X_train:\n",
    "    print(x, 1 if perceptron.predict(x) > .5 else 0, xor(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\n",
    "    a) Implemente una red con aprendizaje Backpropagation que aprenda la siguiente función:\n",
    "    $$\n",
    "    f(x, y, z) = \\sin(x) + \\cos(y) + z\n",
    "    $$\n",
    "    donde $x, y \\in [0, 2\\pi]$ y $z \\in [-1, 1]$.  \n",
    "    Para ello construya un conjunto de datos de entrenamiento y un conjunto de evaluación. Muestre la evolución del error de entrenamiento y de evaluación en función de las épocas de entrenamiento.\n",
    "\n",
    "    b) Estudie la evolución de los errores durante el entrenamiento de una red con una capa oculta de $30$ neuronas cuando el conjunto de entrenamiento contiene $40$ muestras.  \n",
    "    ¿Qué ocurre si el minibatch tiene tamaño 40? ¿Y si tiene tamaño 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, y, z):\n",
    "    return np.sin(x) + np.cos(y) + z\n",
    "\n",
    "X_train = [[x, y, z] for x, y, z in zip(np.linspace(0, 2*np.pi, 100), np.linspace(0, 2*np.pi, 100), np.linspace(-1, 1, 100))]\n",
    "Y_train = np.array([f(x, y, z) for x, y, z in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voy a tener que cambiar la función costo usando el mse\n",
    "# la arquitectura no debe estar tan mal\n",
    "# me está errando los negativos porque la sigmoide sólo me da > 0\n",
    "# voy a usar tangente hiperbólica\n",
    "# la tanh por lo menos no es > 0 pero está acotada < 1 :(\n",
    "\n",
    "perceptron = PerceptronMulticapa([3,15,15,15,1], g=tanh, d_g=d_tanh)\n",
    "perceptron.train(X_train,Y_train,lr=0.01,iters=10000)\n",
    "\n",
    "# for x, y, z in X_train:\n",
    "#     print(str(round(x, 2))+\"\\t\"+\n",
    "#           str(round(y, 2))+\"\\t\"+\n",
    "#           str(round(z, 2))+\"\\t\"+\n",
    "#           str(round(perceptron.predict([x, y, z])[0],2))+\"\\t\"+\n",
    "#           str(round(f(x, y, z),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(Y_h, Y):\n",
    "    return sum((Y_h - Y)**2)/len(Y)\n",
    "\n",
    "Y = np.array([perceptron.predict([x, y, z])[0] for x, y, z in X_train])\n",
    "mse(Y_train, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Y)):\n",
    "    print(round(Y[i],2), round(Y_train[i],2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Siguiendo el trabajo de Hinton y Salakhutdinov (2006), entrene una máquina restringida\n",
    "de Boltzmann con imágenes de la base de datos MNIST. Muestre el error de\n",
    "recontruccion durante el entrenamiento, y ejemplos de cada uno de los dígitos\n",
    "reconstruidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def E(v, h, w, bv, bh):\n",
    "    return -bv @ v -bh @ h - v @ w @ h\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class RBM:\n",
    "    def __init__(self, v_size, h_size):\n",
    "        self.v_size = v_size\n",
    "        self.h_size = h_size\n",
    "        self.v = np.array([0 for _ in range(v_size)])\n",
    "        self.h = np.array([np.random.randn() for _ in range(h_size)])\n",
    "        self.w = np.random.randn(v_size, h_size)\n",
    "        self.bv = np.random.randn(v_size)\n",
    "        self.bh = np.random.randn(h_size)\n",
    "    def learn(self, train, epsilon=0.05, n=300):\n",
    "        for _ in range(n):\n",
    "            f_vh_data = np.zeros((self.v_size, self.h_size)) # fracción de veces que v_i y h_j están activados simultáneamente en los datos\n",
    "            f_vh_recon = np.zeros((self.v_size, self.h_size)) # misma fracción para las confabulaciones\n",
    "            for t in train:\n",
    "                self.v = t\n",
    "                p_h = sigmoid(self.bh + self.v @ self.w)\n",
    "                self.h = np.array([1 if np.random.rand() < p else 0 for p in p_h])\n",
    "                f_vh_data += np.outer(self.v, self.h)\n",
    "                # confabulaciones\n",
    "                p_v = sigmoid(self.bv + self.h @ self.w.T)\n",
    "                self.v = np.array([1 if np.random.rand() < p else 0 for p in p_v])\n",
    "                p_h = sigmoid(self.bh + self.v @ self.w)\n",
    "                self.h = np.array([1 if np.random.rand() < p else 0 for p in p_h])\n",
    "                f_vh_recon += np.outer(self.v, self.h)\n",
    "            f_vh_data /= len(train)\n",
    "            f_vh_recon /= len(train)\n",
    "            # actualizo los pesos\n",
    "            self.w += epsilon * (f_vh_data - f_vh_recon)\n",
    "            # actualizo los sesgos\n",
    "            self.bv += epsilon * (np.mean(train, axis=0) - np.mean([self.v for _ in train], axis=0))\n",
    "            self.bh += epsilon * (np.mean([sigmoid(self.bh + t @ self.w) for t in train], axis=0)\n",
    "                                - np.mean([sigmoid(self.bh + self.v @ self.w) for _ in train], axis=0))\n",
    "    def sample(self, x):\n",
    "        self.v = x\n",
    "        p_h = sigmoid(self.bh + self.v @ self.w)\n",
    "        self.h = np.array([1 if np.random.rand() < p else 0 for p in p_h])\n",
    "        p_v = sigmoid(self.bv + self.h @ self.w.T)\n",
    "        self.v = np.array([1 if np.random.rand() < p else 0 for p in p_v])\n",
    "        return self.h\n",
    "\n",
    "# apilando RBMs\n",
    "class StackedRBMs():\n",
    "    def __init__(self, rbms):\n",
    "        self.rbms = rbms\n",
    "    def learn(self, train, epsilon=0.05, n=300):\n",
    "        v = train\n",
    "        for rbm in self.rbms:\n",
    "            rbm.learn(v, epsilon, n)\n",
    "            v = [rbm.sample(t) for t in v]\n",
    "        # backpropagation\n",
    "    def sample(self, x):\n",
    "        # forward\n",
    "        v = x\n",
    "        for rbm in self.rbms:\n",
    "            v = rbm.sample(v, n)\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "# mnist sin labels\n",
    "train = loadmat(\"data/rbm/datosTrain.mat\")[\"data\"]/255\n",
    "# test = loadmat(\"data/rbm/datosTest.mat\")[\"data\"]/255\n",
    "\n",
    "rbm1 = RBM(784, 392)\n",
    "rbm2 = RBM(392, 196)\n",
    "rbm3 = RBM(196, 392)\n",
    "rbm4 = RBM(392, 784)\n",
    "\n",
    "srbms = StackedRBMs([rbm1, rbm2, rbm3, rbm4])\n",
    "# aprendiendo\n",
    "srbms.learn(train, epsilon=0.05, n=300)\n",
    "\n",
    "y = train[0]\n",
    "# reconstrucción\n",
    "s = srbms.sample(y)\n",
    "\n",
    "# muestro la imagen original y su reconstrucción\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(y.reshape(28, 28))\n",
    "plt.title('Imagen Original')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(s.reshape(28, 28))\n",
    "plt.title('Reconstrucción')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def E(v, h, w, bv, bh):\n",
    "    return -bv @ v -bh @ h - v @ w @ h\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class RBM:\n",
    "    def __init__(self, v_size, h_size):\n",
    "        self.v_size = v_size\n",
    "        self.h_size = h_size\n",
    "        self.v = np.array([0 for _ in range(v_size)])\n",
    "        self.h = np.array([np.random.randn() for _ in range(h_size)])\n",
    "        self.w = np.random.normal(0, 0.01, (v_size, h_size))\n",
    "        self.bv = np.random.randn(v_size)\n",
    "        self.bh = np.random.randn(h_size)\n",
    "    def learn(self, train, epsilon=0.05, n=300):\n",
    "        # repetir el proceso de aprendizaje no supervisado n veces\n",
    "        for _ in range(n):\n",
    "            f_vh_data = np.zeros((self.v_size, self.h_size))  # v_i y h_j activados en los datos\n",
    "            f_vh_recon = np.zeros((self.v_size, self.h_size))  # v_i y h_j activados en reconstrucciones\n",
    "            # estadísticas de los sesgos\n",
    "            data_bv_update = np.zeros(self.v_size)\n",
    "            recon_bv_update = np.zeros(self.v_size)\n",
    "            data_bh_update = np.zeros(self.h_size)\n",
    "            recon_bh_update = np.zeros(self.h_size)\n",
    "            for t in train:\n",
    "                # datos originales\n",
    "                self.v = t\n",
    "                p_h = sigmoid(self.bh + self.v @ self.w)\n",
    "                self.h = np.array([1 if np.random.rand() < p else 0 for p in p_h])\n",
    "                f_vh_data += np.outer(self.v, self.h)\n",
    "                # confabulaciones\n",
    "                p_v = sigmoid(self.bv + self.h @ self.w.T)\n",
    "                recon_v = np.array([1 if np.random.rand() < p else 0 for p in p_v])\n",
    "                p_h_recon = sigmoid(self.bh + recon_v @ self.w)\n",
    "                recon_h = np.array([1 if np.random.rand() < p else 0 for p in p_h_recon])\n",
    "                f_vh_recon += np.outer(recon_v, recon_h)\n",
    "                data_bv_update += t\n",
    "                recon_bv_update += recon_v\n",
    "                data_bh_update += p_h\n",
    "                recon_bh_update += p_h_recon\n",
    "            # normalizo las estadísticas\n",
    "            f_vh_data /= len(train)\n",
    "            f_vh_recon /= len(train)\n",
    "            data_bv_update /= len(train)\n",
    "            recon_bv_update /= len(train)\n",
    "            data_bh_update /= len(train)\n",
    "            recon_bh_update /= len(train)\n",
    "            # acutalizo los pesos y los sesgos\n",
    "            self.w += epsilon * (f_vh_data - f_vh_recon)\n",
    "            self.bv += epsilon * (data_bv_update - recon_bv_update)\n",
    "            self.bh += epsilon * (data_bh_update - recon_bh_update)\n",
    "    def sample(self, x):\n",
    "        self.v = x\n",
    "        p_h = sigmoid(self.bh + self.v @ self.w)\n",
    "        self.h = np.array([1 if np.random.rand() < p else 0 for p in p_h])\n",
    "        p_v = sigmoid(self.bv + self.h @ self.w.T)\n",
    "        self.v = np.array([1 if np.random.rand() < p else 0 for p in p_v])\n",
    "        return self.h\n",
    "\n",
    "# apilando RBMs\n",
    "class StackedRBMs():\n",
    "    def __init__(self, rbms):\n",
    "        self.rbms = rbms\n",
    "    def learn(self, train, epsilon=0.05, n=300):\n",
    "        v = train\n",
    "        for rbm in self.rbms:\n",
    "            rbm.learn(v, epsilon, n)\n",
    "            v = [rbm.sample(t) for t in v]\n",
    "        # backpropagation\n",
    "    def sample(self, x):\n",
    "        # Forward pass\n",
    "        v = x\n",
    "        for rbm in self.rbms:\n",
    "            p_h = sigmoid(rbm.bh + v @ rbm.w)\n",
    "            v = (np.random.rand(*p_h.shape) < p_h).astype(np.float32)\n",
    "        # Backward pass\n",
    "        for rbm in reversed(self.rbms):\n",
    "            p_v = sigmoid(rbm.bv + v @ rbm.w.T)\n",
    "            v = (np.random.rand(*p_v.shape) < p_v).astype(np.float32)\n",
    "        return v\n",
    "\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# mnist sin labels\n",
    "train = loadmat(\"data/rbm/datosTrain.mat\")[\"data\"]/255\n",
    "test = loadmat(\"data/rbm/datosTest.mat\")[\"data\"]/255\n",
    "# por ahora binarizo los datos\n",
    "train = (train > 0.5).astype(np.float32)\n",
    "test = (test > 0.5).astype(np.float32)\n",
    "\n",
    "rbm1 = RBM(784, 392)\n",
    "rbm2 = RBM(392, 196)\n",
    "rbm3 = RBM(196, 392)\n",
    "rbm4 = RBM(392, 784)\n",
    "\n",
    "# inicializo las rbms apiladas\n",
    "srbms = StackedRBMs([rbm1, rbm2, rbm3, rbm4])\n",
    "# aprendiendo\n",
    "srbms.learn(train, epsilon=0.05, n=100)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# muestro las imágenes originales y sus reconstrucciones\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i in range(5):\n",
    "    y = test[i]\n",
    "    s = srbms.sample(y)\n",
    "    # imagen original\n",
    "    axes[0, i].imshow(y.reshape(28, 28), cmap='gray')\n",
    "    axes[0, i].set_title('Original')\n",
    "    axes[0, i].axis('off')\n",
    "    # reconstrucción\n",
    "    axes[1, i].imshow(s.reshape(28, 28), cmap='gray')\n",
    "    axes[1, i].set_title('Reconstrucción')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Entrene una red convolucional para clasificar las imágenes de la base de datos MNIST.  \n",
    "¿Cuál es la red convolucional más pequeña que puede conseguir con una exactitud de al menos 90% en el conjunto de evaluación? ¿Cuál es el perceptrón multicapa más\n",
    "pequeño que puede conseguir con la misma exactitud?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Entrene un autoencoder para obtener una representación de baja dimensionalidad de las\n",
    "imágenes de MNIST. Use dichas representaciones para entrenar un perceptrón\n",
    "multicapa como clasificador. ¿Cuál es el tiempo de entrenamiento y la exactitud del\n",
    "clasificador obtenido cuando parte de la representación del autoencoder, en\n",
    "comparación con lo obtenido usando las imágenes originales?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
