{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implemente un perceptrón simple que aprenda la función lógica $AND$ y la función lógica $OR$, de $2$ y de $4$ entradas. Muestre la evolución del error durante el entrenamiento. Para el caso de $2$ dimensiones, grafique la recta discriminadora y todos los vectores de entrada de la red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/perceptrón-simple1.png)\n",
    "\n",
    "$AND$ de $2$ entradas:\n",
    "| $x_1$ | $0$ | $0$ | $1$ | $1$ |\n",
    "|-------|-----|-----|-----|-----|\n",
    "| $x_2$ | $0$ | $1$ | $0$ | $1$ |\n",
    "| $y$   | $0$ | $0$ | $0$ | $1$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "np.random.seed(2002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AND(X):\n",
    "    return all(X)\n",
    "\n",
    "def OR(X):\n",
    "    return any(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronSimple:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(3)\n",
    "    def train(self, X, Y, alpha, iter_):\n",
    "        for _ in range(iter_):\n",
    "            for n in range(len(X)):\n",
    "                a = self.predict(X[n])\n",
    "                if a != Y[n]:\n",
    "                    self.W[0] += alpha * (Y[n] - a) * X[n][0]\n",
    "                    self.W[1] += alpha * (Y[n] - a) * X[n][1]\n",
    "                    self.W[2] += alpha * (Y[n] - a) * (-1)\n",
    "    def predict(self, x):\n",
    "        h = np.dot(np.append(x, -1), self.W)\n",
    "        return 0 if h < 0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [[x1,x2] for x1 in [0,1] for x2 in[0,1]]\n",
    "Y_train = [AND(x) for x in X_train]\n",
    "\n",
    "perceptron = PerceptronSimple()\n",
    "perceptron.train(X_train[1:] + [[1,1]], Y_train[1:] + [True], 0.01, 10000)\n",
    "\n",
    "for x in X_train:\n",
    "    print(x, perceptron.predict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$AND$ de $4$ entradas:\n",
    "\n",
    "| $x_1$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $1$ | $1$ | $1$ | $1$ | $1$ | $1$ | $1$ | $1$ |\n",
    "|-------|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|\n",
    "| $x_2$ | $0$ | $0$ | $0$ | $0$ | $1$ | $1$ | $1$ | $1$ | $0$ | $0$ | $0$ | $0$ | $1$ | $1$ | $1$ | $1$ |\n",
    "| $x_3$ | $0$ | $0$ | $1$ | $1$ | $0$ | $0$ | $0$ | $1$ | $0$ | $0$ | $1$ | $1$ | $0$ | $0$ | $1$ | $1$ |\n",
    "| $x_4$ | $0$ | $1$ | $0$ | $1$ | $0$ | $1$ | $0$ | $1$ | $0$ | $1$ | $0$ | $1$ | $0$ | $1$ | $0$ | $1$ |\n",
    "| $y$   | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $1$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronSimple:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(5)\n",
    "    def train(self, X, Y, alpha, iter_):\n",
    "        for _ in range(iter_):\n",
    "            for n in range(len(X)):\n",
    "                a = self.predict(X[n])\n",
    "                if a != Y[n]:\n",
    "                    self.W[0] += alpha * (Y[n] - a) * X[n][0]\n",
    "                    self.W[1] += alpha * (Y[n] - a) * X[n][1]\n",
    "                    self.W[2] += alpha * (Y[n] - a) * X[n][2]\n",
    "                    self.W[3] += alpha * (Y[n] - a) * X[n][3]\n",
    "                    self.W[4] += alpha * (Y[n] - a) * (-1)\n",
    "    def predict(self, x):\n",
    "        h = np.dot(np.append(x, -1), self.W)\n",
    "        return 0 if h < 0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [[x1,x2,x3,x4] for x1 in [0,1] for x2 in[0,1] for x3 in [0,1] for x4 in[0,1]]\n",
    "Y_train = [AND(x) for x in X_train]\n",
    "\n",
    "perceptron = PerceptronSimple()\n",
    "perceptron.train(X_train[5:] + [[1,1,1,1] for _ in range(5)], Y_train[5:] + [True for _ in range(5)], 0.001, 10000)\n",
    "\n",
    "for x in X_train:\n",
    "    print(x, perceptron.predict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Implemente un perceptrón multicapa que aprenda la función lógica $XOR$ de $2$ y de $4$ entradas (utilizando el algoritmo Backpropagation y actualizando en batch). Muestre cómo evoluciona el error durante el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/perceptrón-multicapa1.png)\n",
    "\n",
    "$XOR$ de $2$ entradas:\n",
    "| $x_1$ | $0$ | $0$ | $1$ | $1$ |\n",
    "|-------|-----|-----|-----|-----|\n",
    "| $x_2$ | $0$ | $1$ | $0$ | $1$ |\n",
    "| $y$   | $0$ | $1$ | $1$ | $0$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def d_sigmoid(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def d_tanh(z):\n",
    "    return 1 - np.tanh(z)**2\n",
    "\n",
    "class PerceptronMulticapa:\n",
    "    def __init__(self, sizes, g=sigmoid, d_g=d_sigmoid):\n",
    "        self.L = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.a = [[0 for _ in range(s)] for s in sizes]\n",
    "        self.z = [[0 for _ in range(s)] for s in sizes[1:]]\n",
    "        self.w = [np.random.randn(n,m) for n, m in zip(sizes[1:], sizes[:-1])]\n",
    "        self.b = [np.random.randn(s) for s in sizes[1:]]\n",
    "        self.g = g\n",
    "        self.d_g = d_g\n",
    "    def predict(self, x):\n",
    "        self.a[0] = x\n",
    "        for l in range(1, self.L):\n",
    "            self.z[l-1] = self.w[l-1] @ self.a[l-1] + self.b[l-1]\n",
    "            self.a[l] = self.g(self.z[l-1])\n",
    "        return self.a[-1]\n",
    "\n",
    "    def train(self, X, Y, lr=0.01, iters=1000):\n",
    "        for _ in range(iters):  # Iterar varias veces\n",
    "            for x, y in zip(X, Y):\n",
    "                # predigo x para actualizar la matriz de activaciones\n",
    "                self.predict(x)\n",
    "                \n",
    "                # calculo el error\n",
    "                grad_C_a = self.a[-1] - y\n",
    "                delta_l = grad_C_a * self.d_g(self.z[-1])\n",
    "                \n",
    "                # inicializo los gradientes\n",
    "                grad_C_w = [np.zeros_like(w) for w in self.w]\n",
    "                grad_C_b = [np.zeros_like(b) for b in self.b]\n",
    "                \n",
    "                # backprop\n",
    "                for l in range(self.L-2, -1, -1):\n",
    "                    grad_C_w[l] = np.outer(delta_l, self.a[l])\n",
    "                    grad_C_b[l] = delta_l\n",
    "                    if l > 0:\n",
    "                        delta_l = (self.w[l].T @ delta_l) * self.d_g(self.z[l-1])\n",
    "                \n",
    "                # muevo los parámetros en la dirección contraria al gradiente en módulo learning rate\n",
    "                self.w = [w - lr * grad_w for w, grad_w in zip(self.w, grad_C_w)]\n",
    "                self.b = [b - lr * grad_b for b, grad_b in zip(self.b, grad_C_b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [[x1, x2] for x1 in [0, 1] for x2 in [0, 1]]\n",
    "Y_train = [x1 ^ x2 for x1, x2 in X_train]\n",
    "\n",
    "perceptron = PerceptronMulticapa([2,3,1])\n",
    "perceptron.train(X_train, Y_train, lr=0.1, iters=10000)\n",
    "for x in X_train:\n",
    "    print(x, 1 if perceptron.predict(x) > .5 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$XOR$ de $4$ entradas:\n",
    "| $x_1$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $0$ | $1$ | $1$ | $1$ | $1$ | $1$ | $1$ | $1$ | $1$ |\n",
    "|-------|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|\n",
    "| $x_2$ | $0$ | $0$ | $0$ | $0$ | $1$ | $1$ | $1$ | $1$ | $0$ | $0$ | $0$ | $0$ | $1$ | $1$ | $1$ | $1$ |\n",
    "| $x_3$ | $0$ | $0$ | $1$ | $1$ | $0$ | $0$ | $1$ | $1$ | $0$ | $0$ | $1$ | $1$ | $0$ | $0$ | $1$ | $1$ |\n",
    "| $x_4$ | $0$ | $1$ | $0$ | $1$ | $0$ | $1$ | $0$ | $1$ | $0$ | $1$ | $0$ | $1$ | $0$ | $1$ | $0$ | $1$ |\n",
    "| $y$   | $0$ | $1$ | $1$ | $0$ | $1$ | $0$ | $0$ | $1$ | $1$ | $0$ | $0$ | $1$ | $0$ | $1$ | $1$ | $0$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xor(x):\n",
    "    return x[0] ^ x[1] ^ x[2] ^ x[3]\n",
    "\n",
    "X_train = [[x1, x2, x3, x4] for x1 in [0, 1] for x2 in [0, 1] for x3 in [0, 1] for x4 in [0, 1]]\n",
    "Y_train = [xor(x) for x in X_train]\n",
    "\n",
    "perceptron = PerceptronMulticapa([4,8,1])\n",
    "perceptron.train(X_train,Y_train,lr=0.01,iters=100000)\n",
    "for x in X_train:\n",
    "    print(x, 1 if perceptron.predict(x) > .5 else 0, xor(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\n",
    "    a) Implemente una red con aprendizaje Backpropagation que aprenda la siguiente función:\n",
    "    $$\n",
    "    f(x, y, z) = \\sin(x) + \\cos(y) + z\n",
    "    $$\n",
    "    donde $x, y \\in [0, 2\\pi]$ y $z \\in [-1, 1]$.  \n",
    "    Para ello construya un conjunto de datos de entrenamiento y un conjunto de evaluación. Muestre la evolución del error de entrenamiento y de evaluación en función de las épocas de entrenamiento.\n",
    "\n",
    "    b) Estudie la evolución de los errores durante el entrenamiento de una red con una capa oculta de $30$ neuronas cuando el conjunto de entrenamiento contiene $40$ muestras.  \n",
    "    ¿Qué ocurre si el minibatch tiene tamaño 40? ¿Y si tiene tamaño 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, y, z):\n",
    "    return np.sin(x) + np.cos(y) + z\n",
    "\n",
    "X_train = [[x, y, z] for x, y, z in zip(np.linspace(0, 2*np.pi, 100), np.linspace(0, 2*np.pi, 100), np.linspace(-1, 1, 100))]\n",
    "Y_train = np.array([f(x, y, z) for x, y, z in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voy a tener que cambiar la función costo usando el mse\n",
    "# la arquitectura no debe estar tan mal\n",
    "# me está errando los negativos porque la sigmoide sólo me da > 0\n",
    "# voy a usar tangente hiperbólica\n",
    "# la tanh por lo menos no es > 0 pero está acotada < 1 :(\n",
    "\n",
    "perceptron = PerceptronMulticapa([3,15,15,15,1], g=tanh, d_g=d_tanh)\n",
    "perceptron.train(X_train,Y_train,lr=0.01,iters=10000)\n",
    "\n",
    "# for x, y, z in X_train:\n",
    "#     print(str(round(x, 2))+\"\\t\"+\n",
    "#           str(round(y, 2))+\"\\t\"+\n",
    "#           str(round(z, 2))+\"\\t\"+\n",
    "#           str(round(perceptron.predict([x, y, z])[0],2))+\"\\t\"+\n",
    "#           str(round(f(x, y, z),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(Y_h, Y):\n",
    "    return sum((Y_h - Y)**2)/len(Y)\n",
    "\n",
    "Y = np.array([perceptron.predict([x, y, z])[0] for x, y, z in X_train])\n",
    "mse(Y_train, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Y)):\n",
    "    print(round(Y[i],2), round(Y_train[i],2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Siguiendo el trabajo de Hinton y Salakhutdinov (2006), entrene una máquina restringida\n",
    "de Boltzmann con imágenes de la base de datos MNIST. Muestre el error de\n",
    "recontruccion durante el entrenamiento, y ejemplos de cada uno de los dígitos\n",
    "reconstruidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def E(v, h, w, bv, bh):\n",
    "    return -bv @ v -bh @ h - v @ w @ h\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class RBM:\n",
    "    def __init__(self, v_size, h_size):\n",
    "        self.v_size = v_size\n",
    "        self.h_size = h_size\n",
    "        self.v = np.array([0 for _ in range(v_size)])\n",
    "        self.h = np.array([np.random.randn() for _ in range(h_size)])\n",
    "        self.w = np.random.normal(0, 0.01, (v_size, h_size))\n",
    "        self.bv = np.random.randn(v_size)\n",
    "        self.bh = np.random.randn(h_size)\n",
    "    def learn(self, train, epsilon=0.05, n=300):\n",
    "        # repetir el proceso de aprendizaje no supervisado n veces\n",
    "        for _ in range(n):\n",
    "            f_vh_data = np.zeros((self.v_size, self.h_size))  # v_i y h_j activados en los datos\n",
    "            f_vh_recon = np.zeros((self.v_size, self.h_size))  # v_i y h_j activados en reconstrucciones\n",
    "            # estadísticas de los sesgos\n",
    "            data_bv_update = np.zeros(self.v_size)\n",
    "            recon_bv_update = np.zeros(self.v_size)\n",
    "            data_bh_update = np.zeros(self.h_size)\n",
    "            recon_bh_update = np.zeros(self.h_size)\n",
    "            for t in train:\n",
    "                # datos originales\n",
    "                self.v = t\n",
    "                p_h = sigmoid(self.bh + self.v @ self.w)\n",
    "                self.h = np.array([1 if np.random.rand() < p else 0 for p in p_h])\n",
    "                f_vh_data += np.outer(self.v, self.h)\n",
    "                # confabulaciones\n",
    "                p_v = sigmoid(self.bv + self.h @ self.w.T)\n",
    "                recon_v = np.array([1 if np.random.rand() < p else 0 for p in p_v])\n",
    "                p_h_recon = sigmoid(self.bh + recon_v @ self.w)\n",
    "                recon_h = np.array([1 if np.random.rand() < p else 0 for p in p_h_recon])\n",
    "                f_vh_recon += np.outer(recon_v, recon_h)\n",
    "                data_bv_update += t\n",
    "                recon_bv_update += recon_v\n",
    "                data_bh_update += p_h\n",
    "                recon_bh_update += p_h_recon\n",
    "            # normalizo las estadísticas\n",
    "            f_vh_data /= len(train)\n",
    "            f_vh_recon /= len(train)\n",
    "            data_bv_update /= len(train)\n",
    "            recon_bv_update /= len(train)\n",
    "            data_bh_update /= len(train)\n",
    "            recon_bh_update /= len(train)\n",
    "            # acutalizo los pesos y los sesgos\n",
    "            self.w += epsilon * (f_vh_data - f_vh_recon)\n",
    "            self.bv += epsilon * (data_bv_update - recon_bv_update)\n",
    "            self.bh += epsilon * (data_bh_update - recon_bh_update)\n",
    "    def sample(self, x):\n",
    "        self.v = x\n",
    "        p_h = sigmoid(self.bh + self.v @ self.w)\n",
    "        self.h = np.array([1 if np.random.rand() < p else 0 for p in p_h])\n",
    "        # p_v = sigmoid(self.bv + self.h @ self.w.T)\n",
    "        # self.v = np.array([1 if np.random.rand() < p else 0 for p in p_v])\n",
    "        # ra datos continuos, la actualización es samplear una gaussiana con varianza unitaria y media b_i + sum_j h_j w_ij\n",
    "        self.v = np.array(np.random.normal(self.bv + self.h @ self.w.T, 1, self.v_size))\n",
    "        return self.h\n",
    "    def forward(self, x):\n",
    "        v = x\n",
    "        p_h = sigmoid(self.bh + v @ self.w)\n",
    "        v = (np.random.rand(*p_h.shape) < p_h).astype(np.float32)\n",
    "        return v\n",
    "    def backward(self, x):\n",
    "        v = x\n",
    "        p_v = sigmoid(self.bv + v @ self.w.T)\n",
    "        v = (np.random.rand(*p_v.shape) < p_v).astype(np.float32)\n",
    "        return v\n",
    "\n",
    "# apilando RBMs\n",
    "class StackedRBMs():\n",
    "    def __init__(self, rbms):\n",
    "        self.rbms = rbms\n",
    "    def learn(self, train, epsilon=0.05, n=300):\n",
    "        v = train\n",
    "        for rbm in self.rbms:\n",
    "            rbm.learn(v, epsilon, n)\n",
    "            v = [rbm.sample(t) for t in v]\n",
    "    def sample(self, x):\n",
    "        # forward\n",
    "        v = x\n",
    "        for rbm in self.rbms:\n",
    "            v = rbm.forward(v)\n",
    "        # backward\n",
    "        for rbm in reversed(self.rbms):\n",
    "            v = rbm.backward(v)\n",
    "        return v\n",
    "\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# mnist sin labels\n",
    "train = loadmat(\"data/rbm/datosTrain.mat\")[\"data\"]/255\n",
    "test = loadmat(\"data/rbm/datosTest.mat\")[\"data\"]/255\n",
    "# por ahora binarizo los datos\n",
    "# train = (train > 0.5).astype(np.float32)\n",
    "# test = (test > 0.5).astype(np.float32)\n",
    "\n",
    "rbm1 = RBM(784, 392)\n",
    "rbm2 = RBM(392, 196)\n",
    "rbm3 = RBM(196, 392)\n",
    "rbm4 = RBM(392, 784)\n",
    "\n",
    "# inicializo las rbms apiladas\n",
    "srbms = StackedRBMs([rbm1, rbm2, rbm3, rbm4])\n",
    "# aprendiendo\n",
    "srbms.learn(train, epsilon=0.05, n=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# muestro las imágenes originales y sus reconstrucciones\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i in range(5):\n",
    "    y = test[i]\n",
    "    s = srbms.sample(y)\n",
    "    # imagen original\n",
    "    axes[0, i].imshow(y.reshape(28, 28), cmap='gray')\n",
    "    axes[0, i].set_title('Original')\n",
    "    axes[0, i].axis('off')\n",
    "    # reconstrucción\n",
    "    axes[1, i].imshow(s.reshape(28, 28), cmap='gray')\n",
    "    axes[1, i].set_title('Reconstrucción')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Entrene una red convolucional para clasificar las imágenes de la base de datos MNIST.  \n",
    "¿Cuál es la red convolucional más pequeña que puede conseguir con una exactitud de al menos 90% en el conjunto de evaluación? ¿Cuál es el perceptrón multicapa más\n",
    "pequeño que puede conseguir con la misma exactitud?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Entrene un autoencoder para obtener una representación de baja dimensionalidad de las\n",
    "imágenes de MNIST. Use dichas representaciones para entrenar un perceptrón\n",
    "multicapa como clasificador. ¿Cuál es el tiempo de entrenamiento y la exactitud del\n",
    "clasificador obtenido cuando parte de la representación del autoencoder, en\n",
    "comparación con lo obtenido usando las imágenes originales?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
